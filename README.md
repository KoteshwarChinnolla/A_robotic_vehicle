

## This is the source code of the paper

[Natural Language Based Autonomous Navigation Vehicle](https://ieeexplore.ieee.org/document/11118685)

```
C. Koteshwar, M. D. Chaithanya, M. Abdul Kaleem and A. Vishnubhatla, "Natural Language Based Autonomous Navigation Vehicle," 2025 International Conference on Computing Technologies (ICOCT), Bengaluru, India, 2025, pp. 1-7, doi: 10.1109/ICOCT64433.2025.11118685. keywords: {Location awareness;Computer vision;Visualization;Laser radar;Navigation;Computational modeling;Natural languages;Brain modeling;Motors;Collision avoidance;LLM;SSH;VNC},
```

## Functionalities of the Vehicle
> 1. Understanding Natural language
> 2. Transforming things from one place to another
> 3. Responding to User inputs
> 4. asks task-specific questions
> 5. performing complex and lengthy workflows flawlessly

## Understanding Natural language

This is a crucial step in the generation of workflow plans and responses.
We use LLMs with tool-calling features to understand the user response accurately and make plans accordingly. We make use of LLM in different ways
> 1. Tool-calling LLM (for Generation of task-specific workflows)
> 2. Input Classification LLM ( for classifying whether the user is asking to perform a Task or the user question refers to a general response like the current position of the vehicle or history of tasks it has performed till now or any functionality related)
> 3. Orchestrator (For making user Input a bit more constructed to make perfect plans)
> 4. Responding LLM ( for responding to the input message given by user. It doesn't refer to location-specific messages)

## Transferring things from one place to another

Here we use a combination of the A* algorithm and decoder.

**A* algorithm** - It is a path-finding algorithm. it is highly accurate in finding the shortest path  between 2 nodes in graph-based environments. In our case, we consider our image as a graph where obstacles are represented by 1s and free paths are represented by 0. We make use of the A* algorithm to find the shortest path between the nodes given by the user(through LLMs)

**Decoder** - The decoder is specifically used to make the outputs generated by the A* algorithm in a way that can be instructed to Arduino so that the vehicle follows that specific path accurately without any directionality errors.

## Responding to user Inputs

There are 3 major reasons for implementing this functionality. 

> 1. What if the vehicle is performing another task while the user requests his task?
> 2. What if there is no such location in the house or industry environment?
> 3. What if the user wants to clarify things like (where it is currently or the history of tasks it has performed till now or any functionality related) rather then making it to perform the task?

In every case, the user question is redirected to Responding LLM. so that it process user input and generate responses accordingly.

## Asks Task Specific questions

currently, the vehicle itself doesn't perform tasks by itself it takes the help of Humans.

The functionality of the vehicle is to Travel, transport and speak. so every time it is performing tasks if the vehicle can't do it by itself it asks for someone who help it out in performing that task.

To make you understand let us consider an example - if the user asks to bring some coffee from the kitchen to the bedroom.  so it first reaches the kitchen and asks for someone who makes the coffee. it weighs until the coffee is done making. then it comes back to the bedroom with the coffee.

## Performing complex and lengthy workflows flawlessly.

**Complex? What complex over here? it is just travelling from one place to another place..** you may get dought like this but I want you to imagene some cases>

> ### *Case 1.*
>   **task in task** (tasks are dependent). if it needed to complete some task which is part of its original task.
> 
> for better understanding let's have an example - same as the previous if someone wants coffee from the kitchen to bedroom . As the vehicle reaches the kitchen when it asks to make coffee, what if the user says that **there is no coffee powder go to the storeroom and bring it back to the kitchen**.
> 
> In this case it has to perform task in task. where it has to make a plan to go to the store room and bring the coffee powder back, then it needs to travel to the bedroom with the coffee as the coffee making is done.
> 
> For implementation I used the concept of **Backtracking**

> ### *case 2*
>  **Task After Task** (tasks are independent): When should Task 2 be completed if it is instructed to start while Task 1 is still in progress?
>
> example: if the vehicle is in the process of bringing coffee to the bedroom if someone instructs it to get the bat from the storeroom to the park. as the tasks are not related to each other it needs to serialise them. one after the other.
>
> to make this happen I used the concept of serialisation.


# To run it on your local system
Follow the steps below to run the robotic vehicle project on your local system using Docker.

## Prerequisites

 - Docker installed on your system

 - Docker Desktop running (Docker Engine must be active)

> **Step 1** : Install Docker for your operating system if itâ€™s not already installed.
> 
> **Step 2** : Clone the project repository using Git:
```python
git clone https://github.com/KoteshwarChinnolla/A_robotic_vehicle
```
> **Step 3**: " Create a .env file in the root directory and add your configuration details.
```python
ARDUINO_PORT=""   # Specify the Arduino port if connected (optional)
                 # Examples:
                 # Windows: COM7
                 # Linux: /dev/ttyUSB0

GROQ_API_KEY=""   # Your Groq API key
GROQ_MODEL=""     # Optional: default is llama-3.3-70b-versatile
```

You can leave ARDUINO_PORT empty if no Arduino is connected.

> **Step 4**: Ensure Docker Desktop is running and the Docker Engine is active.
> 
> **Step 5**:run
```python
docker compose up --build
```


## Direct Docker Commands

### Docker Command for Local Machine (without Arduino)

If you're running the container on your local machine and don't have an Arduino, you can use the following command:

```bash
docker run -d --name vehicle_real -p 5000:5000 --env GROQ_API_KEY=your_groq_api_key_here koti21/robotic_car:v0.0.1
```


### Docker Command for Raspberry Pi (with Arduino)

If you're running the container on a Raspberry Pi and have an Arduino connected, use the following command:

```bash
docker run -d --name vehicle_real -p 5000:5000 --env GROQ_API_KEY=your_groq_api_key_here --env ARDUINO_PORT=/dev/ttyUSB0 koti21/robotic_car:v0.0.1
```

Make sure to replace `your_groq_api_key_here` with your actual `GROQ_API_KEY` and update the `ARDUINO_PORT` to match the correct port where your Arduino is connected.

You can also specify the model you would like to use
just add the `GROQ_MODEL` default will be llama-3.3-70b-versatile


# Inference

After a successful run:
1. Open your browser
2. Navigate to: 
```bash
http://localhost:5000/ui/
```
3. select an image
4. Type or speak your thoughts to interact with the system